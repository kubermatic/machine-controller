{
  "ignition": {
    "config": {},
    "timeouts": {},
    "version": "2.1.0"
  },
  "networkd": {
    "units": [
      {
        "contents": "[Match]\n# Because of difficulty predicting specific NIC names on different cloud providers,\n# we only support static addressing on VSphere. There should be a single NIC attached\n# that we will match by name prefix 'en' which denotes ethernet devices.\nName=en*\n\n[Network]\nDHCP=no\nAddress=192.168.81.4/24\nGateway=192.168.81.1\nDNS=8.8.8.8\n",
        "name": "static-nic.network"
      }
    ]
  },
  "passwd": {
    "users": [
      {
        "name": "core",
        "sshAuthorizedKeys": [
          "ssh-rsa AAABBB",
          "ssh-rsa CCCDDD"
        ]
      }
    ]
  },
  "storage": {
    "files": [
      {
        "filesystem": "root",
        "group": {},
        "path": "/etc/systemd/journald.conf.d/max_disk_use.conf",
        "user": {},
        "contents": {
          "source": "data:,%5BJournal%5D%0ASystemMaxUse%3D5G%0A",
          "verification": {}
        },
        "mode": 420
      },
      {
        "filesystem": "root",
        "group": {},
        "path": "/etc/sysctl.d/k8s.conf",
        "user": {},
        "contents": {
          "source": "data:,kernel.panic_on_oops%20%3D%201%0Akernel.panic%20%3D%2010%0Avm.overcommit_memory%20%3D%201%0A",
          "verification": {}
        },
        "mode": 420
      },
      {
        "filesystem": "root",
        "group": {},
        "path": "/proc/sys/kernel/panic_on_oops",
        "user": {},
        "contents": {
          "source": "data:,1%0A",
          "verification": {}
        },
        "mode": 420
      },
      {
        "filesystem": "root",
        "group": {},
        "path": "/proc/sys/kernel/panic",
        "user": {},
        "contents": {
          "source": "data:,10%0A",
          "verification": {}
        },
        "mode": 420
      },
      {
        "filesystem": "root",
        "group": {},
        "path": "/proc/sys/vm/overcommit_memory",
        "user": {},
        "contents": {
          "source": "data:,1%0A",
          "verification": {}
        },
        "mode": 420
      },
      {
        "filesystem": "root",
        "group": {},
        "path": "/etc/kubernetes/bootstrap.kubeconfig",
        "user": {},
        "contents": {
          "source": "data:,apiVersion%3A%20v1%0Aclusters%3A%0A-%20cluster%3A%0A%20%20%20%20certificate-authority-data%3A%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%0A%20%20%20%20server%3A%20https%3A%2F%2Fserver%3A443%0A%20%20name%3A%20%22%22%0Acontexts%3A%20%5B%5D%0Acurrent-context%3A%20%22%22%0Akind%3A%20Config%0Apreferences%3A%20%7B%7D%0Ausers%3A%0A-%20name%3A%20%22%22%0A%20%20user%3A%0A%20%20%20%20token%3A%20my-token%0A",
          "verification": {}
        },
        "mode": 256
      },
      {
        "filesystem": "root",
        "group": {},
        "path": "/etc/kubernetes/cloud-config",
        "user": {},
        "contents": {
          "source": "data:,my%0Acustom%0Acloud-config%0A",
          "verification": {}
        },
        "mode": 256
      },
      {
        "filesystem": "root",
        "group": {},
        "path": "/etc/kubernetes/ca.crt",
        "user": {},
        "contents": {
          "source": "data:,-----BEGIN%20CERTIFICATE-----%0AMIIEWjCCA0KgAwIBAgIJALfRlWsI8YQHMA0GCSqGSIb3DQEBBQUAMHsxCzAJBgNV%0ABAYTAlVTMQswCQYDVQQIEwJDQTEWMBQGA1UEBxMNU2FuIEZyYW5jaXNjbzEUMBIG%0AA1UEChMLQnJhZGZpdHppbmMxEjAQBgNVBAMTCWxvY2FsaG9zdDEdMBsGCSqGSIb3%0ADQEJARYOYnJhZEBkYW5nYS5jb20wHhcNMTQwNzE1MjA0NjA1WhcNMTcwNTA0MjA0%0ANjA1WjB7MQswCQYDVQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBG%0AcmFuY2lzY28xFDASBgNVBAoTC0JyYWRmaXR6aW5jMRIwEAYDVQQDEwlsb2NhbGhv%0Ac3QxHTAbBgkqhkiG9w0BCQEWDmJyYWRAZGFuZ2EuY29tMIIBIjANBgkqhkiG9w0B%0AAQEFAAOCAQ8AMIIBCgKCAQEAt5fAjp4fTcekWUTfzsp0kyih1OYbsGL0KX1eRbSS%0AR8Od0%2B9Q62Hyny%2BGFwMTb4A%2FKU8mssoHvcceSAAbwfbxFK%2F%2Bs51TobqUnORZrOoT%0AZjkUygbyXDSK99YBbcR1Pip8vwMTm4XKuLtCigeBBdjjAQdgUO28LENGlsMnmeYk%0AJfODVGnVmr5Ltb9ANA8IKyTfsnHJ4iOCS%2FPlPbUj2q7YnoVLposUBMlgUb%2FCykX3%0AmOoLb4yJJQyA%2FiST6ZxiIEj36D4yWZ5lg7YJl%2BUiiBQHGCnPdGyipqV06ex0heYW%0AcaiW8LWZSUQ93jQ%2BWVCH8hT7DQO1dmsvUmXlq%2FJeAlwQ%2FQIDAQABo4HgMIHdMB0G%0AA1UdDgQWBBRcAROthS4P4U7vTfjByC569R7E6DCBrQYDVR0jBIGlMIGigBRcAROt%0AhS4P4U7vTfjByC569R7E6KF%2FpH0wezELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkNB%0AMRYwFAYDVQQHEw1TYW4gRnJhbmNpc2NvMRQwEgYDVQQKEwtCcmFkZml0emluYzES%0AMBAGA1UEAxMJbG9jYWxob3N0MR0wGwYJKoZIhvcNAQkBFg5icmFkQGRhbmdhLmNv%0AbYIJALfRlWsI8YQHMAwGA1UdEwQFMAMBAf8wDQYJKoZIhvcNAQEFBQADggEBAG6h%0AU9f9sNH0%2F6oBbGGy2EVU0UgITUQIrFWo9rFkrW5k%2FXkDjQm%2B3lzjT0iGR4IxE%2FAo%0AeU6sQhua7wrWeFEn47GL98lnCsJdD7oZNhFmQ95Tb%2FLnDUjs5Yj9brP0NWzXfYU4%0AUK2ZnINJRcJpB8iRCaCxE8DdcUF0XqIEq6pA272snoLmiXLMvNl3kYEdm%2Bje6voD%0A58SNVEUsztzQyXmJEhCpwVI0A6QCjzXj%2Bqvpmw3ZZHi8JwXei8ZZBLTSFBki8Z7n%0AsH9BBH38%2FSzUmAN4QHSPy1gjqm00OAE8NaYDkh%2FbzE4d7mLGGMWp%2FWE3KPSu82HF%0AkPe6XoSbiLm%2Fkxk32T0%3D%0A-----END%20CERTIFICATE-----%0A",
          "verification": {}
        },
        "mode": 420
      },
      {
        "filesystem": "root",
        "group": {},
        "path": "/etc/hostname",
        "user": {},
        "contents": {
          "source": "data:,node1",
          "verification": {}
        },
        "mode": 384
      },
      {
        "filesystem": "root",
        "group": {
          "id": 0
        },
        "path": "/etc/ssh/sshd_config",
        "user": {
          "id": 0
        },
        "contents": {
          "source": "data:,%23%20Use%20most%20defaults%20for%20sshd%20configuration.%0ASubsystem%20sftp%20internal-sftp%0AClientAliveInterval%20180%0AUseDNS%20no%0AUsePAM%20yes%0APrintLastLog%20no%20%23%20handled%20by%20PAM%0APrintMotd%20no%20%23%20handled%20by%20PAM%0APasswordAuthentication%20no%0AChallengeResponseAuthentication%20no%0A",
          "verification": {}
        },
        "mode": 384
      },
      {
        "filesystem": "root",
        "group": {},
        "path": "/etc/systemd/system/docker.service.d/10-storage.conf",
        "user": {},
        "contents": {
          "source": "data:,%5BService%5D%0AEnvironment%3DDOCKER_OPTS%3D--storage-driver%3Doverlay2%0A",
          "verification": {}
        },
        "mode": 420
      },
      {
        "filesystem": "root",
        "group": {},
        "path": "/opt/bin/health-monitor.sh",
        "user": {},
        "contents": {
          "source": "data:,%23!%2Fusr%2Fbin%2Fenv%20bash%0A%0A%23%20Copyright%202016%20The%20Kubernetes%20Authors.%0A%23%0A%23%20Licensed%20under%20the%20Apache%20License%2C%20Version%202.0%20(the%20%22License%22)%3B%0A%23%20you%20may%20not%20use%20this%20file%20except%20in%20compliance%20with%20the%20License.%0A%23%20You%20may%20obtain%20a%20copy%20of%20the%20License%20at%0A%23%0A%23%20%20%20%20%20http%3A%2F%2Fwww.apache.org%2Flicenses%2FLICENSE-2.0%0A%23%0A%23%20Unless%20required%20by%20applicable%20law%20or%20agreed%20to%20in%20writing%2C%20software%0A%23%20distributed%20under%20the%20License%20is%20distributed%20on%20an%20%22AS%20IS%22%20BASIS%2C%0A%23%20WITHOUT%20WARRANTIES%20OR%20CONDITIONS%20OF%20ANY%20KIND%2C%20either%20express%20or%20implied.%0A%23%20See%20the%20License%20for%20the%20specific%20language%20governing%20permissions%20and%0A%23%20limitations%20under%20the%20License.%0A%0A%23%20This%20script%20is%20for%20master%20and%20node%20instance%20health%20monitoring%2C%20which%20is%0A%23%20packed%20in%20kube-manifest%20tarball.%20It%20is%20executed%20through%20a%20systemd%20service%0A%23%20in%20cluster%2Fgce%2Fgci%2F%3Cmaster%2Fnode%3E.yaml.%20The%20env%20variables%20come%20from%20an%20env%0A%23%20file%20provided%20by%20the%20systemd%20service.%0A%0Aset%20-o%20nounset%0Aset%20-o%20pipefail%0A%0A%23%20We%20simply%20kill%20the%20process%20when%20there%20is%20a%20failure.%20Another%20systemd%20service%20will%0A%23%20automatically%20restart%20the%20process.%0Afunction%20container_runtime_monitoring%20%7B%0A%20%20local%20-r%20max_attempts%3D5%0A%20%20local%20attempt%3D1%0A%20%20local%20-r%20crictl%3D%22%24%7BKUBE_HOME%7D%2Fbin%2Fcrictl%22%0A%20%20local%20-r%20container_runtime_name%3D%22%24%7BCONTAINER_RUNTIME_NAME%3A-docker%7D%22%0A%20%20%23%20We%20still%20need%20to%20use%20'docker%20ps'%20when%20container%20runtime%20is%20%22docker%22.%20This%20is%20because%0A%20%20%23%20dockershim%20is%20still%20part%20of%20kubelet%20today.%20When%20kubelet%20is%20down%2C%20crictl%20pods%0A%20%20%23%20will%20also%20fail%2C%20and%20docker%20will%20be%20killed.%20This%20is%20undesirable%20especially%20when%0A%20%20%23%20docker%20live%20restore%20is%20disabled.%0A%20%20local%20healthcheck_command%3D%22docker%20ps%22%0A%20%20if%20%5B%5B%20%22%24%7BCONTAINER_RUNTIME%3A-docker%7D%22%20!%3D%20%22docker%22%20%5D%5D%3B%20then%0A%20%20%20%20healthcheck_command%3D%22%24%7Bcrictl%7D%20pods%22%0A%20%20fi%0A%20%20%23%20Container%20runtime%20startup%20takes%20time.%20Make%20initial%20attempts%20before%20starting%0A%20%20%23%20killing%20the%20container%20runtime.%0A%20%20until%20timeout%2060%20%24%7Bhealthcheck_command%7D%20%3E%20%2Fdev%2Fnull%3B%20do%0A%20%20%20%20if%20((%20attempt%20%3D%3D%20max_attempts%20))%3B%20then%0A%20%20%20%20%20%20echo%20%22Max%20attempt%20%24%7Bmax_attempts%7D%20reached!%20Proceeding%20to%20monitor%20container%20runtime%20healthiness.%22%0A%20%20%20%20%20%20break%0A%20%20%20%20fi%0A%20%20%20%20echo%20%22%24attempt%20initial%20attempt%20%5C%22%24%7Bhealthcheck_command%7D%5C%22!%20Trying%20again%20in%20%24attempt%20seconds...%22%0A%20%20%20%20sleep%20%22%24((%202%20**%20attempt%2B%2B%20))%22%0A%20%20done%0A%20%20while%20true%3B%20do%0A%20%20%20%20if%20!%20timeout%2060%20%24%7Bhealthcheck_command%7D%20%3E%20%2Fdev%2Fnull%3B%20then%0A%20%20%20%20%20%20echo%20%22Container%20runtime%20%24%7Bcontainer_runtime_name%7D%20failed!%22%0A%20%20%20%20%20%20if%20%5B%5B%20%22%24container_runtime_name%22%20%3D%3D%20%22docker%22%20%5D%5D%3B%20then%0A%20%20%20%20%20%20%20%20%20%20%23%20Dump%20stack%20of%20docker%20daemon%20for%20investigation.%0A%20%20%20%20%20%20%20%20%20%20%23%20Log%20fle%20name%20looks%20like%20goroutine-stacks-TIMESTAMP%20and%20will%20be%20saved%20to%0A%20%20%20%20%20%20%20%20%20%20%23%20the%20exec%20root%20directory%2C%20which%20is%20%2Fvar%2Frun%2Fdocker%2F%20on%20Ubuntu%20and%20COS.%0A%20%20%20%20%20%20%20%20%20%20pkill%20-SIGUSR1%20dockerd%0A%20%20%20%20%20%20fi%0A%20%20%20%20%20%20systemctl%20kill%20--kill-who%3Dmain%20%22%24%7Bcontainer_runtime_name%7D%22%0A%20%20%20%20%20%20%23%20Wait%20for%20a%20while%2C%20as%20we%20don't%20want%20to%20kill%20it%20again%20before%20it%20is%20really%20up.%0A%20%20%20%20%20%20sleep%20120%0A%20%20%20%20else%0A%20%20%20%20%20%20sleep%20%22%24%7BSLEEP_SECONDS%7D%22%0A%20%20%20%20fi%0A%20%20done%0A%7D%0A%0Afunction%20kubelet_monitoring%20%7B%0A%20%20echo%20%22Wait%20for%202%20minutes%20for%20kubelet%20to%20be%20functional%22%0A%20%20%23%20TODO(andyzheng0831)%3A%20replace%20it%20with%20a%20more%20reliable%20method%20if%20possible.%0A%20%20sleep%20120%0A%20%20local%20-r%20max_seconds%3D10%0A%20%20local%20output%3D%22%22%0A%20%20while%20%5B%201%20%5D%3B%20do%0A%20%20%20%20if%20!%20output%3D%24(curl%20-m%20%22%24%7Bmax_seconds%7D%22%20-f%20-s%20-S%20http%3A%2F%2F127.0.0.1%3A10248%2Fhealthz%202%3E%261)%3B%20then%0A%20%20%20%20%20%20%23%20Print%20the%20response%20and%2For%20errors.%0A%20%20%20%20%20%20echo%20%24output%0A%20%20%20%20%20%20echo%20%22Kubelet%20is%20unhealthy!%22%0A%20%20%20%20%20%20systemctl%20kill%20kubelet%0A%20%20%20%20%20%20%23%20Wait%20for%20a%20while%2C%20as%20we%20don't%20want%20to%20kill%20it%20again%20before%20it%20is%20really%20up.%0A%20%20%20%20%20%20sleep%2060%0A%20%20%20%20else%0A%20%20%20%20%20%20sleep%20%22%24%7BSLEEP_SECONDS%7D%22%0A%20%20%20%20fi%0A%20%20done%0A%7D%0A%0A%0A%23%23%23%23%23%23%23%23%23%23%23%23%23%23%20Main%20Function%20%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%23%0Aif%20%5B%5B%20%22%24%23%22%20-ne%201%20%5D%5D%3B%20then%0A%20%20echo%20%22Usage%3A%20health-monitor.sh%20%3Ccontainer-runtime%2Fkubelet%3E%22%0A%20%20exit%201%0Afi%0A%0AKUBE_HOME%3D%22%2Fhome%2Fkubernetes%22%0A%0ASLEEP_SECONDS%3D10%0Acomponent%3D%241%0Aecho%20%22Start%20kubernetes%20health%20monitoring%20for%20%24%7Bcomponent%7D%22%0Aif%20%5B%5B%20%22%24%7Bcomponent%7D%22%20%3D%3D%20%22container-runtime%22%20%5D%5D%3B%20then%0A%20%20container_runtime_monitoring%0Aelif%20%5B%5B%20%22%24%7Bcomponent%7D%22%20%3D%3D%20%22kubelet%22%20%5D%5D%3B%20then%0A%20%20kubelet_monitoring%0Aelse%0A%20%20echo%20%22Health%20monitoring%20for%20component%20%22%24%7Bcomponent%7D%22%20is%20not%20supported!%22%0Afi%0A",
          "verification": {}
        },
        "mode": 755
      }
    ]
  },
  "systemd": {
    "units": [
      {
        "mask": true,
        "name": "update-engine.service"
      },
      {
        "mask": true,
        "name": "locksmithd.service"
      },
      {
        "enabled": true,
        "name": "docker.service"
      },
      {
        "contents": "[Unit]\nRequires=network-online.target\nAfter=network-online.target\n\n[Service]\nExecStart=/opt/bin/health-monitor.sh kubelet\n\n[Install]\nWantedBy=multi-user.target\n",
        "enabled": true,
        "name": "kubelet-healthcheck.service"
      },
      {
        "contents": "[Unit]\nRequires=network-online.target\nAfter=network-online.target\n\n[Service]\nExecStart=/opt/bin/health-monitor.sh container-runtime\n\n[Install]\nWantedBy=multi-user.target\n",
        "enabled": true,
        "name": "docker-healthcheck.service"
      },
      {
        "contents": "[Unit]\nDescription=Kubernetes Kubelet\nRequires=docker.service\nAfter=docker.service\n[Service]\nTimeoutStartSec=5min\nEnvironment=KUBELET_IMAGE=docker://k8s.gcr.io/hyperkube-amd64:v1.12.0\nEnvironment=\"RKT_RUN_ARGS=--uuid-file-save=/var/cache/kubelet-pod.uuid \\\n  --insecure-options=image \\\n  --volume=resolv,kind=host,source=/etc/resolv.conf \\\n  --mount volume=resolv,target=/etc/resolv.conf \\\n  --volume cni-bin,kind=host,source=/opt/cni/bin \\\n  --mount volume=cni-bin,target=/opt/cni/bin \\\n  --volume cni-conf,kind=host,source=/etc/cni/net.d \\\n  --mount volume=cni-conf,target=/etc/cni/net.d \\\n  --volume etc-kubernetes,kind=host,source=/etc/kubernetes \\\n  --mount volume=etc-kubernetes,target=/etc/kubernetes \\\n  --volume var-log,kind=host,source=/var/log \\\n  --mount volume=var-log,target=/var/log \\\n  --volume var-lib-calico,kind=host,source=/var/lib/calico \\\n  --mount volume=var-lib-calico,target=/var/lib/calico\"\nExecStartPre=/bin/mkdir -p /var/lib/calico\nExecStartPre=/bin/mkdir -p /etc/kubernetes/manifests\nExecStartPre=/bin/mkdir -p /etc/cni/net.d\nExecStartPre=/bin/mkdir -p /opt/cni/bin\nExecStartPre=-/usr/bin/rkt rm --uuid-file=/var/cache/kubelet-pod.uuid\nExecStart=/usr/lib/coreos/kubelet-wrapper \\\n  --container-runtime=docker \\\n  --allow-privileged=true \\\n  --cni-bin-dir=/opt/cni/bin \\\n  --cni-conf-dir=/etc/cni/net.d \\\n  --cluster-dns=10.10.10.10 \\\n  --cluster-domain=cluster.local \\\n  --authentication-token-webhook=true \\\n  --hostname-override=node1 \\\n  --network-plugin=cni \\\n  --cloud-provider=vsphere \\\n  --cloud-config=/etc/kubernetes/cloud-config \\\n  --cert-dir=/etc/kubernetes/ \\\n  --pod-manifest-path=/etc/kubernetes/manifests \\\n  --resolv-conf=/etc/resolv.conf \\\n  --rotate-certificates=true \\\n  --kubeconfig=/etc/kubernetes/kubeconfig \\\n  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\\n  --lock-file=/var/run/lock/kubelet.lock \\\n  --exit-on-lock-contention \\\n  --read-only-port=0 \\\n  --protect-kernel-defaults=true \\\n  --authorization-mode=Webhook \\\n  --anonymous-auth=false \\\n  --client-ca-file=/etc/kubernetes/ca.crt\nExecStop=-/usr/bin/rkt stop --uuid-file=/var/cache/kubelet-pod.uuid\nRestart=always\nRestartSec=10\n[Install]\nWantedBy=multi-user.target\n",
        "dropins": [
          {
            "contents": "[Unit]\nRequires=docker.service\nAfter=docker.service\n",
            "name": "40-docker.conf"
          }
        ],
        "enabled": true,
        "name": "kubelet.service"
      }
    ]
  }
}